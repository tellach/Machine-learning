{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP06 : Naïve Bayes\n",
    "\n",
    "Tout le monde connait le théorème de Bayes pour calculer la probabilité conditionnelle d'un évennement $A$ sachant un autre $B$: \n",
    "$$ P(A|B) = \\frac{P(A)P(B|A)}{P(B)}$$\n",
    "\n",
    "Pour appliquer ce théorème sur un problème d'appentissage automatique, l'idée est simple ; Etant donné une caractéristique $f$ et la sortie $y$ qui peut avoir la classe $c$ : \n",
    "- Remplacer $A$ par $y=c$\n",
    "- Remplacer $B$ par $f$ \n",
    "On aura l'équation : \n",
    "$$ P(y=c|f) = \\frac{P(y=c)P(f|y=c)}{P(f)}$$\n",
    "\n",
    "On appelle : \n",
    "- $P(y=c|f)$ postérieure \n",
    "- $P(y=c)$ antérieure\n",
    "- $P(f|y=c)$ vraisemblance\n",
    "- $P(f)$ évidence \n",
    "\n",
    "Ici, on estime la probablité d'une classe $c$ sachant une caractéristique $f$ en utilisant des données d'entrainement. Maintenant, on veut estimer la probabilité d'une classe $c$ sachant un vecteur de caractéristiques $\\overrightarrow{f} = \\{f_1, ..., f_L\\}$ : \n",
    "$$ P(y=c|\\overrightarrow{f}) = \\frac{P(y=c)P(\\overrightarrow{f}|y=c)}{P(f)}$$\n",
    "\n",
    "Etant donnée plusieurs classes $c_j$, la classe choisie $\\hat{c}$ est celle avec la probabilité maximale \n",
    "$$\\hat{c} = \\arg\\max\\limits_{c_k} P(y=c_k|\\overrightarrow{f})$$\n",
    "$$\\hat{c} = \\arg\\max\\limits_{c_k} \\frac{P(y=c_k)P(\\overrightarrow{f}|y=c_k)}{P(f)}$$\n",
    "On supprime l'évidence pour cacher le crime : $P(f)$ ne dépend pas de $c_k$ et elle est postive, donc ça ne va pas affecter la fonction $\\max$.\n",
    "$$\\hat{c} = \\arg\\max\\limits_{c_k} P(y=c_k)P(\\overrightarrow{f}|y=c_k)$$\n",
    "\n",
    "Pour calculer $P(\\overrightarrow{f}|y=c_k)$, on va utiliser une properiété naïve (d'où vient le nom Naive Bayes) : on suppose l'indépendence conditionnelle entre les caractéristiques $f_j$. \n",
    "$$\\hat{c} = \\arg\\max\\limits_{c_k} P(y=c_k) \\prod\\limits_{f_j \\in \\overrightarrow{f}} P(f_j|y=c_k)$$\n",
    "\n",
    "Pour éviter la disparition de la probabilité (multiplication et représentation de virgule flottante sur machine), on transforme vers l'espace logarithme.\n",
    "$$\\hat{c} = \\arg\\max\\limits_{c_k} \\log P(y=c_k) + \\sum\\limits_{f_j \\in \\overrightarrow{f}} \\log P(f_j|y=c_k)$$\n",
    "\n",
    "\n",
    "## Avantages \n",
    "\n",
    "Les classifieurs naïfs bayésiens, malgré leur simplicité, ont des points forts:\n",
    "- Ils ont besoin d'une petite quantité de données d'entrainement.\n",
    "- Ils sont très rapides par rapport aux autres classifieurs.\n",
    "- Ils donnent de bons résultats dans le cas de filtrage du courrier indésirable et de classification de documents.\n",
    "\n",
    "## Limites\n",
    "Les classifieurs naïfs bayésiens certes sont populaires à cause de leur simplicité. Mais, une telle simplicité vient avec un coût [référence: Spiderman].\n",
    "- Les probabilités obtenues en utilisant ces classifieurs ne doivent pas être prises au sérieux.\n",
    "- S'il existe une grande corrélation entre les caractéristiques, ils vont donner une mauvaise performance.\n",
    "- Dans le cas des caractéristiques continues (prix, surface, etc.), les données doivent suivre la loi normale.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I- Implémentation\n",
    "\n",
    "Pour estimer la vraisemblance, il y a plusieurs modèles (lois):\n",
    "- Loi multinomiale : pour les caracétristiques nominales\n",
    "- Loi de Bernoulli : lorsqu'on est interressé par l'apparence d'une caractéristique ou non (binaire)\n",
    "- loi normale : pour les caractéristiques numériques\n",
    "\n",
    "Dans ce TP, on va implémenter Naive Bayes pour les caractéristiques nominales (loi multinomiale)\n",
    "\n",
    "### I-1- Les données pour les tests unitaires\n",
    "Ici, on va utiliser le dataset \"jouer\" contenant des caractéristiques nominales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>temps</th>\n",
       "      <th>temperature</th>\n",
       "      <th>humidite</th>\n",
       "      <th>vent</th>\n",
       "      <th>jouer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>ensoleile</td>\n",
       "      <td>chaude</td>\n",
       "      <td>haute</td>\n",
       "      <td>non</td>\n",
       "      <td>non</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>ensoleile</td>\n",
       "      <td>chaude</td>\n",
       "      <td>haute</td>\n",
       "      <td>oui</td>\n",
       "      <td>non</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>nuageux</td>\n",
       "      <td>chaude</td>\n",
       "      <td>haute</td>\n",
       "      <td>non</td>\n",
       "      <td>oui</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>pluvieux</td>\n",
       "      <td>douce</td>\n",
       "      <td>haute</td>\n",
       "      <td>non</td>\n",
       "      <td>oui</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>pluvieux</td>\n",
       "      <td>fraiche</td>\n",
       "      <td>normale</td>\n",
       "      <td>non</td>\n",
       "      <td>oui</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>pluvieux</td>\n",
       "      <td>fraiche</td>\n",
       "      <td>normale</td>\n",
       "      <td>oui</td>\n",
       "      <td>non</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>nuageux</td>\n",
       "      <td>fraiche</td>\n",
       "      <td>normale</td>\n",
       "      <td>oui</td>\n",
       "      <td>oui</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>ensoleile</td>\n",
       "      <td>douce</td>\n",
       "      <td>haute</td>\n",
       "      <td>non</td>\n",
       "      <td>non</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>ensoleile</td>\n",
       "      <td>fraiche</td>\n",
       "      <td>normale</td>\n",
       "      <td>non</td>\n",
       "      <td>oui</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>pluvieux</td>\n",
       "      <td>douce</td>\n",
       "      <td>normale</td>\n",
       "      <td>non</td>\n",
       "      <td>oui</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>ensoleile</td>\n",
       "      <td>douce</td>\n",
       "      <td>normale</td>\n",
       "      <td>oui</td>\n",
       "      <td>oui</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>nuageux</td>\n",
       "      <td>douce</td>\n",
       "      <td>haute</td>\n",
       "      <td>oui</td>\n",
       "      <td>oui</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>nuageux</td>\n",
       "      <td>chaude</td>\n",
       "      <td>normale</td>\n",
       "      <td>non</td>\n",
       "      <td>oui</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>pluvieux</td>\n",
       "      <td>douce</td>\n",
       "      <td>haute</td>\n",
       "      <td>oui</td>\n",
       "      <td>non</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        temps temperature humidite vent jouer\n",
       "0   ensoleile      chaude    haute  non   non\n",
       "1   ensoleile      chaude    haute  oui   non\n",
       "2     nuageux      chaude    haute  non   oui\n",
       "3    pluvieux       douce    haute  non   oui\n",
       "4    pluvieux     fraiche  normale  non   oui\n",
       "5    pluvieux     fraiche  normale  oui   non\n",
       "6     nuageux     fraiche  normale  oui   oui\n",
       "7   ensoleile       douce    haute  non   non\n",
       "8   ensoleile     fraiche  normale  non   oui\n",
       "9    pluvieux       douce  normale  non   oui\n",
       "10  ensoleile       douce  normale  oui   oui\n",
       "11    nuageux       douce    haute  oui   oui\n",
       "12    nuageux      chaude  normale  non   oui\n",
       "13   pluvieux       douce    haute  oui   non"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "\n",
    "\n",
    "jouer = pd.read_csv(\"datasets/jouer.csv\")\n",
    "\n",
    "X_jouer = jouer.iloc[:, :-1].values # Premières colonnes \n",
    "Y_jouer = jouer.iloc[:,-1].values # Dernière colonne \n",
    "\n",
    "# Afficher le dataset \"jouer\"\n",
    "jouer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I-2- Estimation de la probabilité antérieure\n",
    "Etant donné le vecteur de sortie $Y$, on doit calculer la probabilité de chaque classe (différentes valeurs de $Y$)\n",
    "\n",
    "$$p(c_k) = \\frac{|\\{y / y \\in Y \\text{ et } y = c_k\\}|}{|Y|}$$\n",
    "\n",
    "La fonction doit retourner un dictionnaire où la clé est le nom de la classe et la valeur est sa probabilité. Voici, un exemple d'un dictionnaire dans Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(Iris-setosa)= 0.5\n",
      "P(Iris-versicolor)= 0.33\n",
      "P(Iris-virginica)= 0.67\n"
     ]
    }
   ],
   "source": [
    "# Exemple de dictionnaire dans Python \n",
    "d = {}\n",
    "d[\"Iris-setosa\"] = 0.5\n",
    "d[\"Iris-versicolor\"] = 0.33\n",
    "d[\"Iris-virginica\"] = 0.67\n",
    "\n",
    "for c in d: \n",
    "    print(\"P(\" + c + \")= \" + str(d[c]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'non': 0.35714285714285715, 'oui': 0.6428571428571429}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO Réaliser la fonction \n",
    "def P_c(Y): \n",
    "    vals = np.unique(Y)\n",
    "    resultat = {}\n",
    "    for i in range(vals.shape[0]):\n",
    "        resultat[vals[i]] = np.sum(Y == vals[i]) / Y.shape[0]\n",
    "    return resultat\n",
    "\n",
    "# Résultat: {'non': 0.35714285714285715, 'oui': 0.6428571428571429}\n",
    "P_c(Y_jouer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I-3- Entrainement  (loi multinomiale)\n",
    "\n",
    "Notre modèle (notons le par $\\theta_{f_j,C}$) doit garder le nombre des différentes valeurs dans une caractéristique $A$ et le nombre de ces valeurs dans chaque classe.\n",
    "\n",
    "Donc, étant donné un vecteur d'une caractéristique $A$ et un autre des $Y$ respectives, la fonction d'entrainement doit retourner un dictionnaire (notre théta) : \n",
    "- la clé est une valeur $a_v$ de $A$ \n",
    "- la valeur est un autre dictionnaire : \n",
    "   - il doit contenir une clé \"_total_\" dont la valeur est le nombre d'occurence de $a_v$ dans $A$ \n",
    "   - la clé est la classe $c_k$ de $Y$\n",
    "   - la valeur est le nombre d'occurence de $a_v$ respectives à $c_k$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ensoleile': {'_Total_': 5, 'non': 3, 'oui': 2},\n",
       " 'nuageux': {'_Total_': 4, 'non': 0, 'oui': 4},\n",
       " 'pluvieux': {'_Total_': 5, 'non': 2, 'oui': 3}}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO Réaliser cette fonction \n",
    "# Elle génère théta pour une seule caractéristique\n",
    "def entrainer_multi_1(A, Y): \n",
    "    valsA = np.unique(A)\n",
    "    valsY = np.unique(Y)\n",
    "    \n",
    "    resultat = {}\n",
    "    for i in range (valsA.shape[0]):\n",
    "        result = {}\n",
    "        result['_Total_'] = np.sum(A == valsA[i])\n",
    "        for j in range (valsY.shape[0]):\n",
    "            somme = 0\n",
    "            for k in range (A.shape[0]):\n",
    "                if (Y[k] == valsY[j] and A[k] == valsA[i]):\n",
    "                    somme += 1\n",
    "            result[valsY[j]] = somme\n",
    "        resultat[valsA[i]] = result\n",
    "    return resultat\n",
    "\n",
    "# Résultat \n",
    "# {'ensoleile': {'_total_': 5, 'non': 3, 'oui': 2},\n",
    "# 'nuageux': {'_total_': 4, 'non': 0, 'oui': 4},\n",
    "# 'pluvieux': {'_total_': 5, 'non': 2, 'oui': 3}}\n",
    "Theta_jouer_temps = entrainer_multi_1(X_jouer[:, 0], Y_jouer)\n",
    "\n",
    "Theta_jouer_temps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'ensoleile': {'_Total_': 5, 'non': 3, 'oui': 2},\n",
       "  'nuageux': {'_Total_': 4, 'non': 0, 'oui': 4},\n",
       "  'pluvieux': {'_Total_': 5, 'non': 2, 'oui': 3}},\n",
       " {'chaude': {'_Total_': 4, 'non': 2, 'oui': 2},\n",
       "  'douce': {'_Total_': 6, 'non': 2, 'oui': 4},\n",
       "  'fraiche': {'_Total_': 4, 'non': 1, 'oui': 3}},\n",
       " {'haute': {'_Total_': 7, 'non': 4, 'oui': 3},\n",
       "  'normale': {'_Total_': 7, 'non': 1, 'oui': 6}},\n",
       " {'non': {'_Total_': 8, 'non': 2, 'oui': 6},\n",
       "  'oui': {'_Total_': 6, 'non': 3, 'oui': 3}},\n",
       " {'non': 0.35714285714285715, 'oui': 0.6428571428571429}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# La fonction qui entraine Théta sur plusieurs caractéristiques\n",
    "# Rien à programmer ici\n",
    "# Notre théta est une liste des dictionnaires;\n",
    "# chaque dictionnaire contient le théta de la caractéristique respective à la colonne de X\n",
    "# On ajoute les probabilités antérieures des classes à la fin de résultat\n",
    "def entrainer_multi(X, Y): \n",
    "    resultat = []\n",
    "    for i in range(X.shape[1]): \n",
    "        resultat.append(entrainer_multi_1(X[:, i], Y))\n",
    "    resultat.append(P_c(Y))\n",
    "    return resultat\n",
    "\n",
    "Theta_jouer = entrainer_multi(X_jouer, Y_jouer)\n",
    "\n",
    "Theta_jouer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I-4- Estimation de la probabilité de vraissemblance (loi multinomiale)\n",
    "L'équation pour estimer la vraisemblance \n",
    "$$ P(f_j=v|y=c_k) = \\frac{|\\{ y \\in Y / y = c_k \\text{ et } f_j = v\\}|}{|\\{y = c_k\\}|}$$\n",
    "\n",
    "Si, dans le dataset de test, on veut calculer la probabilité d'une valeur $v$ qui n'existe pas dans le dataset d'entrainnement ou qui n'existe pas pour une classe donnée, on aura une probabilité nulle. Ici, on doit appliquer une fonction de lissage qui donne une petite probabilité aux données non vues dans l'entrainnement. Le lissage qu'on va utiliser est celui de Lidstone. Lorsque $\\alpha = 1$ on l'appelle lissage de Laplace.\n",
    "$$ P(f_j=v|y=c_k) = \\frac{|\\{ y \\in Y / y = c_k \\text{ et } f_j = v\\}| + \\alpha}{|\\{y = c_k\\}| + \\alpha * |V|}$$\n",
    "Où: \n",
    "- $\\alpha$ est une valeur donnée \n",
    "- $V$ est l'ensemble des différentes valeurs de $f_j$ (le vocabulaire)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO compléter cette fonction\n",
    "def P_vraiss_multi(Theta_j, v, c, alpha=1.): \n",
    "    len_V = len(Theta_j) # La taille du vocabulaire\n",
    "    nbr_c = 0\n",
    "    for i in Theta_j.keys() :\n",
    "        nbr_c += Theta_j[i][c]\n",
    "    return None\n",
    "\n",
    "# La probabilité de jouer si temps = pluvieux \n",
    "# P(temps = pluvieux | jouer=oui) = (nbr(temps=pluvieux et jouer=oui)+alpha)/(nbr(jour=oui) + alpha * nbr_diff(temps)))\n",
    "# P(temps = pluvieux | jouer=oui) = (3 + 1)/(9 + 3) ==> 3 est le nombre de différentes valeurs de temps (entrainnement)\n",
    "# P(temps = pluvieux | jouer=oui) = 4/12 ==> 0.33333333333333333333333333333333333~\n",
    "\n",
    "# La probabilité de jouer si temps = neigeux \n",
    "# P(temps = neigeux | jouer=oui) = (nbr(temps=neigeux et jouer=oui)+alpha)/(nbr(jouer=oui) + alpha * nbr_diff(temps)))\n",
    "# P(temps = neigeux | jouer=oui) = (0 + 1)/(9 + 3) ==> 3 est le nombre de différentes valeurs de temps (entrainnement)\n",
    "# P(temps = neigeux | jouer=oui) = 1/13 ==> 0.0833333333333333333333333333333333333~\n",
    "\n",
    "\n",
    "P_vraiss_multi(Theta_jouer_temps, \"pluvieux\", \"oui\"), P_vraiss_multi(Theta_jouer_temps, \"neigeux\", \"oui\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I-5- Prédiction de la classe (loi multinomiale)\n",
    "Revenons maintenant à notre équation de prédiction \n",
    "$$\\hat{c} = \\arg\\max\\limits_{c_k} \\log P(y=c_k) + \\sum\\limits_{f_j \\in \\overrightarrow{f}} \\log P(f_j|y=c_k)$$\n",
    "\n",
    "Ici, vous devez prédire un seule échantillon $x$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO compléter ce code\n",
    "# Pour récupérer le théta de la caractéristique n°0 : Theta[0]\n",
    "# anter est un booléen, si il est False, on ne compte pas la probabilité antérieure P(y = c_k)\n",
    "def predire(x, Theta, alpha=1., anter=True): \n",
    "    c_opt = \"\" # la classe optimale\n",
    "    p_c = Theta[-1] #les classes et leurs probabilités antérieures\n",
    "    if not anter: # si on ne veut pas ajouter les probabiliés antérieures\n",
    "        p_c = dict.fromkeys(p_c, 1.) # on définit le tous en 1; log(1) = 0\n",
    "    max_log_p = np.NINF # - infinity \n",
    "    # compléter ici\n",
    "    return c_opt, max_log_p\n",
    "\n",
    "# Résultat: (('oui', -4.102643365036796), ('oui', -3.6608106127577567))\n",
    "predire([\"pluvieux\", \"fraiche\", \"normale\", \"oui\"], Theta_jouer), predire([\"pluvieux\", \"fraiche\", \"normale\", \"oui\"], Theta_jouer, anter=False) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I-7- Regrouper en une classe (loi multinomiale)\n",
    "\n",
    "**Rien à programmer ici, il y a une petite analyse**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NBMultinom(object): \n",
    "    \n",
    "    def __init__(self, alpha=1.): \n",
    "        self.alpha = alpha\n",
    "        \n",
    "    def entrainer(self, X, Y):\n",
    "        self.Theta = entrainer_multi(X, Y)\n",
    "    \n",
    "    def predire(self, X, anter=True, prob=False): \n",
    "        Y_pred = []\n",
    "        for i in range(len(X)): \n",
    "            c, p = predire(X[i,:], self.Theta, alpha=self.alpha, anter=anter)\n",
    "            if prob:\n",
    "                Y_pred.append(p)\n",
    "            else:\n",
    "                Y_pred.append(c)\n",
    "        return Y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On va entrainer un modèle en utilisant notre imlémentation avec et sans probabilité antérieure. \n",
    "Normalement, on doit tester sur des données non vues (des données qu'on n'a pas utilisé pour l'entrainement). Mais, ici, on va tester sur les mêmes données d'entrainement afin de savoir si le modèle a bien représenté ce dataset ou non (calculer l'erreur) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "notre_modele = NBMultinom()\n",
    "notre_modele.entrainer(X_jouer, Y_jouer)\n",
    "Y_notre_ant = notre_modele.predire(X_jouer)\n",
    "Y_notre_sans_ant = notre_modele.predire(X_jouer, anter=False)\n",
    "\n",
    "# Ici, ce n'ai pas la peine d'exécuter plusieurs fois\n",
    "# puisque le résultat sera le même \n",
    "\n",
    "# Le rapport de classification\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(\"Notre modèle avec probabilité antérieure (a priori)\")\n",
    "print(classification_report(Y_notre_ant, Y_jouer))\n",
    "\n",
    "print(\"Notre modèle sans probabilité antérieure (a priori)\")\n",
    "print(classification_report(Y_notre_sans_ant, Y_jouer))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Analyser les résultats** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II- Détection de spam \n",
    "\n",
    "Ici, on va essayer d'appliquer l'apprentissage automatique sur la détection de spam. \n",
    "Chaque message dans le dataset est représenté en utilisant un modèle \"Sac à mots\" (BoW : Bag of Words).\n",
    "Dans l'entrainement, on récupère les différents mots qui s'apparaissent dans les messages. \n",
    "Chaque mot va être considéré comme une caractéristique. \n",
    "Donc, pour chaque message, la valeur de la caractéristique est la fréquence de son mot dans le message. \n",
    "Par exemple, si le mot \"good\" apparait 3 fois dans le message, donc la caractéristique \"good\" aura la valeur 3 dans ce message.\n",
    "\n",
    "Notre implémentation n'est pas adéquate pour la nature de ce problème. \n",
    "Dans Scikit-learn, le [sklearn.naive_bayes.CategoricalNB](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.CategoricalNB.html) est similaire à notre implémentation. \n",
    "L'algorithme adéquat pour ce type de problème est [sklearn.naive_bayes.MultinomialNB](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html).\n",
    "\n",
    "Le dataset utilisé est [SMS Spam Collection Dataset](https://www.kaggle.com/uciml/sms-spam-collection-dataset).\n",
    "Les algorithmes comparés :\n",
    "- Naive Bayes\n",
    "- Arbre de décision\n",
    "- Regression logistique \n",
    "\n",
    "### II-1- Préparation de données\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = pd.read_csv(\"datasets/spam.csv\", encoding=\"latin-1\")\n",
    "messages = messages.rename(columns={\"v1\": \"classe\", \"v2\": \"texte\"})\n",
    "messages = messages.filter([\"texte\", \"classe\"])\n",
    "\n",
    "messages.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II-2- Entrainement et test des modèles sur plusieurs exécutions \n",
    "\n",
    "Afin de satisfaire un étudiant qui réclame toujours sur le manque des données, nous avons décidé de comparer les algorithmes sur plusieurs excécutions (runs). \n",
    "\n",
    "**Rien à analyser ici**\n",
    "\n",
    "**P.S.** timeit.default_timer() est dépendante du système d'exploitation. Aussi, elle peut être affectée par d'autre processus en parallèle. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "#from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import timeit\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "\n",
    "NBR_RUN = 7\n",
    "\n",
    "temps_train = {\n",
    "    \"naive_bayes\" : [],\n",
    "    \"arbre_decision\": [],\n",
    "    \"reg_log\": []\n",
    "}\n",
    "\n",
    "temps_test = {\n",
    "    \"naive_bayes\" : [],\n",
    "    \"arbre_decision\": [],\n",
    "    \"reg_log\": []\n",
    "}\n",
    "\n",
    "perf = {\n",
    "    \"naive_bayes_P\" : [],\n",
    "    \"arbre_decision_P\": [],\n",
    "    \"reg_log_P\": [], \n",
    "    \"naive_bayes_R\" : [],\n",
    "    \"arbre_decision_R\": [],\n",
    "    \"reg_log_R\": []\n",
    "}\n",
    "\n",
    "\n",
    "for run in range(NBR_RUN): \n",
    "    # prétaitement des données\n",
    "    msg_train, msg_test, Y_train, Y_test = train_test_split(messages[\"texte\"],messages[\"classe\"],test_size=0.2)\n",
    "    count_vectorizer = CountVectorizer()\n",
    "    X_train = count_vectorizer.fit_transform(msg_train)\n",
    "    X_test = count_vectorizer.transform(msg_test)\n",
    "    \n",
    "    # ==================================\n",
    "    # ENTRAINEMENT \n",
    "    # ==================================\n",
    "    \n",
    "    #entrainement Naive Bayes\n",
    "    naive_bayes = MultinomialNB()\n",
    "    temps_debut = timeit.default_timer()\n",
    "    naive_bayes.fit(X_train, Y_train)\n",
    "    temps_train[\"naive_bayes\"].append(timeit.default_timer() - temps_debut)\n",
    "    \n",
    "    #entrainement CART\n",
    "    arbre_decision = DecisionTreeClassifier()\n",
    "    temps_debut = timeit.default_timer()\n",
    "    arbre_decision.fit(X_train, Y_train)\n",
    "    temps_train[\"arbre_decision\"].append(timeit.default_timer() - temps_debut)\n",
    "    \n",
    "    #entrainement Régression logitique\n",
    "    reg_log = LogisticRegression(solver=\"lbfgs\") #solver=sag est plus lent; donc j'ai choisi le plus rapide\n",
    "    temps_debut = timeit.default_timer()\n",
    "    reg_log.fit(X_train, Y_train)\n",
    "    temps_train[\"reg_log\"].append(timeit.default_timer() - temps_debut)\n",
    "    \n",
    "    # ==================================\n",
    "    # TEST \n",
    "    # ==================================\n",
    "    \n",
    "    #test Naive Bayes\n",
    "    temps_debut = timeit.default_timer()\n",
    "    Y_naive_bayes = naive_bayes.predict(X_test)\n",
    "    temps_test[\"naive_bayes\"].append(timeit.default_timer() - temps_debut)\n",
    "    \n",
    "    \n",
    "    #test CART\n",
    "    temps_debut = timeit.default_timer()\n",
    "    Y_arbre_decision = arbre_decision.predict(X_test)\n",
    "    temps_test[\"arbre_decision\"].append(timeit.default_timer() - temps_debut)\n",
    "    \n",
    "    #test Régression logitique\n",
    "    temps_debut = timeit.default_timer()\n",
    "    Y_reg_log = reg_log.predict(X_test)\n",
    "    temps_test[\"reg_log\"].append(timeit.default_timer() - temps_debut)\n",
    "    \n",
    "    # ==================================\n",
    "    # PERFORMANCE \n",
    "    # ==================================\n",
    "    # Ici, on va considérer une classification binaire avec une seule classe \"spam\" \n",
    "    # On ne juge pas le classifieur sur sa capacité de détecter les non spams\n",
    "    \n",
    "    perf[\"naive_bayes_P\"].append(precision_score(Y_test, Y_naive_bayes, pos_label=\"spam\"))\n",
    "    perf[\"arbre_decision_P\"].append(precision_score(Y_test, Y_arbre_decision, pos_label=\"spam\"))\n",
    "    perf[\"reg_log_P\"].append(precision_score(Y_test, Y_reg_log, pos_label=\"spam\"))\n",
    "    \n",
    "    perf[\"naive_bayes_R\"].append(recall_score(Y_test, Y_naive_bayes, pos_label=\"spam\"))\n",
    "    perf[\"arbre_decision_R\"].append(recall_score(Y_test, Y_arbre_decision, pos_label=\"spam\"))\n",
    "    perf[\"reg_log_R\"].append(recall_score(Y_test, Y_reg_log, pos_label=\"spam\"))\n",
    "    \n",
    "    \n",
    "\n",
    "temps_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II-3- Analyse du temps d'apprentissage \n",
    "\n",
    "Combien de temps chaque algorithme prend pour entrainer le même dataset d'entrainement\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(temps_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Analyser**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II-4- Analyse du temps de test \n",
    "\n",
    "Combien de temps chaque algorithme prend pour prédir les classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(temps_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Analyser**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II-5- Analyse de la performance \n",
    "\n",
    "Ici, on compare les modèles en se basant sur leurs capacités à détecter le spam. \n",
    "On va utiliser la précision et le rappel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(perf, columns = [\"arbre_decision_P\", \"naive_bayes_P\", \"reg_log_P\", \"arbre_decision_R\", \"naive_bayes_R\", \"reg_log_R\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Analyser**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
