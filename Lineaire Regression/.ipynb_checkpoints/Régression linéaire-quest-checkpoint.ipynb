{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP 02 : Régression linéaire \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I Régression linéaire à une seule variable \n",
    "Dans cette partie, on commence par implémenter la régression linéaire avec une seule variable de prédiction (predictor). Nous allons donc essayer de résoudre le fameux problème de prédiction du prix d'une maison en connaissant sa superficie. \n",
    "\n",
    "### I.1 Préparation des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt \n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### I.1.1 Lecture des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Superficie</th>\n",
       "      <th>Prix</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>2104</td>\n",
       "      <td>399900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1600</td>\n",
       "      <td>329900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2400</td>\n",
       "      <td>369000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1416</td>\n",
       "      <td>232000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>3000</td>\n",
       "      <td>539900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Superficie    Prix\n",
       "0        2104  399900\n",
       "1        1600  329900\n",
       "2        2400  369000\n",
       "3        1416  232000\n",
       "4        3000  539900"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "header = [\"Superficie\", \"Prix\"]\n",
    "houses = pd.read_csv(\"datasets/houses.csv\", names=header)\n",
    "houses.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Superficie</th>\n",
       "      <th>Prix</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>count</td>\n",
       "      <td>47.000000</td>\n",
       "      <td>47.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>mean</td>\n",
       "      <td>2000.680851</td>\n",
       "      <td>340412.659574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>std</td>\n",
       "      <td>794.702354</td>\n",
       "      <td>125039.899586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>min</td>\n",
       "      <td>852.000000</td>\n",
       "      <td>169900.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25%</td>\n",
       "      <td>1432.000000</td>\n",
       "      <td>249900.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50%</td>\n",
       "      <td>1888.000000</td>\n",
       "      <td>299900.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75%</td>\n",
       "      <td>2269.000000</td>\n",
       "      <td>384450.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>max</td>\n",
       "      <td>4478.000000</td>\n",
       "      <td>699900.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Superficie           Prix\n",
       "count    47.000000      47.000000\n",
       "mean   2000.680851  340412.659574\n",
       "std     794.702354  125039.899586\n",
       "min     852.000000  169900.000000\n",
       "25%    1432.000000  249900.000000\n",
       "50%    1888.000000  299900.000000\n",
       "75%    2269.000000  384450.000000\n",
       "max    4478.000000  699900.000000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "houses.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### I.1.2 Normalisation \n",
    "La normalisation est la mise à echelle des valeurs des caractéristiques. Exemple simple de but : En calculant la distance euclidienne une des caracteristiques va avoir plus d'effet sur le résultat si ses valeurs sont beaucoup plus grandes que celle de l'autre variable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(None, None, None)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Normalisation\n",
    "# TODO: compléter la fonction qui applique le Z-score sur une matrice \n",
    "# La fonction doit retourner 3 résultats \n",
    "# 1: la matrice normalisée, 2: la moyenne, 3: la déviation standard \n",
    "def normalise(X): \n",
    "    row_sums = X.sum(axis=1) # array([ 9, 36, 63])\n",
    "    new_matrix = numpy.zeros((3,3))\n",
    "    for i, (row, row_sum) in enumerate(zip(a, row_sums)):\n",
    "    new_matrix[i,:] = row / row_sum\n",
    "    return new_matrix\n",
    "\n",
    "\n",
    "# Doit afficher: \n",
    "\"\"\"\n",
    "(array([[-0.9166985 ,  0.1833397 ],\n",
    "        [-0.9166985 ,  1.55838744],\n",
    "        [-0.9166985 ,  1.00836835]]), 4.333333333333333, 3.636237371545238)\n",
    "\"\"\"\n",
    "normalise([ [1., 5.],\n",
    "            [1., 10.],\n",
    "            [1., 8]]\n",
    "         )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### I.1.3 Fractionnement des données \n",
    "Ici, on va diviser les données en données de test et d'entrainnement \n",
    "\n",
    "On prend juste les 7 dernières ligne pour le  test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extraction des caractéristiques \n",
    "def fractionner(df):\n",
    "    X = df.iloc[:, :-1].values # Premières colonnes \n",
    "    Y = df.iloc[:,-1].values # Dernière colonne \n",
    "    \n",
    "    # On définit la matrice X comme etant la supérficie concatenée à un vecteur de 1 \n",
    "    # pour faciliter l'algorithme pour theta0 \n",
    "    ones = np.ones([X.shape[0],1])\n",
    "    X = np.concatenate((ones, X), axis=1)\n",
    "    return X, Y\n",
    "\n",
    "# Randomization des données pour marquer les 80% lignes\n",
    "msk = np.random.rand(len(houses)) < 0.8 \n",
    "\n",
    "X_train, Y_train = fractionner(houses[msk])\n",
    "X_test, Y_test = fractionner(houses[~msk])\n",
    "\n",
    "X_train.shape, Y_train.shape, X_test.shape, Y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Les données d'entrainnement en bleu\n",
    "plt.scatter(X_train[:,1], Y_train, color=\"blue\")\n",
    "# Les données de test en rouge\n",
    "plt.scatter(X_test[:,1], Y_test, color=\"red\")\n",
    "plt.xlabel('Superficie')\n",
    "plt.ylabel('Prix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I.2 Descente du gradient \n",
    "\n",
    "#### I.2.1 Définir les paramètres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Définir les hyperparamétres : \n",
    "LEARNING_RATE = 0.01 \n",
    "NB_ITER = 500\n",
    "\n",
    "# TODO :  Initialiser aléatoirement les paramètres :\n",
    "## theta est une liste contenant les deux paramètres theta0 et theta1 \n",
    "theta = None\n",
    "# Affichage des paramètres initiales (5 premières lignes)\n",
    "theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### I.2.2 Définir les fonctions nécessaires pour la regression linéaire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Prédire la valeur : \n",
    "def predire(X, theta):\n",
    "    return None\n",
    "\n",
    "# tester la prédiction \n",
    "X_t = np.array([[1., 5.], [1., 10.], [1., 8]])\n",
    "theta_t = np.array([0.25, 0.5])\n",
    "# Le résulat doit être\n",
    "\"\"\"\n",
    "array([2.75, 5.25, 4.25])\n",
    "\"\"\"\n",
    "predire(X_t, theta_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Définir la fonction de cout : \n",
    "def J(X, Y, theta):\n",
    "    return None\n",
    "\n",
    "# tester le cout \n",
    "#X_t = np.array([[1., 5.], [1., 10.], [1., 8]])\n",
    "#theta_t = np.array([0.25, 0.5])\n",
    "Y_t = np.array([3., 5., 4.5])\n",
    "# Le résulat doit être ((3-2.75)^2 + (5-5.25)^2 + (4.5-4.25)^2))/6\n",
    "\"\"\"\n",
    "0.03125\n",
    "\"\"\"\n",
    "J(X_t, Y_t, theta_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Définir la fonction de gradient :\n",
    "def gradient(X, Y, theta):\n",
    "    return  None\n",
    "# Tester le gradient, le résultat doit être\n",
    "\"\"\"\n",
    "array([-0.08333333, -0.25      ])\n",
    "\"\"\"\n",
    "gradient(X_t, Y_t, theta_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def afficher_droite(X, Y, theta):\n",
    "    plt.scatter(X[:,1], Y, color=\"blue\")\n",
    "    plt.plot(X[:,1], predire(X, theta), color=\"red\")\n",
    "    plt.show()\n",
    "# tester avec X_t et Y_t\n",
    "afficher_droite(X_t, Y_t, theta_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO : Définir la déscente du gradient\n",
    "# Cette fonction doit afficher \n",
    "def gradient_descent(X, Y, theta, nb_iter, learning_rate, affich = True):\n",
    "    #l'historique des couts\n",
    "    couts = []\n",
    "    # on recopie theta pour ne pas modifier celle en entrée\n",
    "    theta1 = theta.copy()\n",
    "    couts.append(J(X, Y, theta1))\n",
    "    \n",
    "    if (affich):\n",
    "        afficher_droite(X, Y, theta1)\n",
    "    \n",
    "    # TODO Définir l'algorithme de la descente de gradient : \n",
    "    \n",
    "        \n",
    "    if(affich):\n",
    "        afficher_droite(X, Y, theta1)\n",
    "\n",
    "    return theta1, couts\n",
    "\n",
    "# Tester la fonction\n",
    "# array([0.25202765, 0.50363117])\n",
    "# [0.03125, 0.030768749999999997, 0.030682227021604936])\n",
    "gradient_descent(X_t, Y_t, theta_t, 3, LEARNING_RATE, affich=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### I.2.3  Appliquer la descente sur nos données \n",
    "Lorsqu'on applique le code suivant, on aura un problème\n",
    "\n",
    "**Question** : Quel est le problème ?\n",
    "Régler le"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Réponse (Quel est le problème)** : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: avant d'appliquer le gradient, il faut faire un traitement \n",
    "\n",
    "theta_optimaux, couts = gradient_descent(X_train, Y_train, theta, NB_ITER, LEARNING_RATE)\n",
    "\n",
    "print(\"Thetas optimaux : \", theta_optimaux)\n",
    "print(couts[-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(couts)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1** : \n",
    "\n",
    "Pour cette exemple, quel est selon vous le nombre d'itérations nécessaires pour obtenir la convergence dans l'algorithme de la descente du gradient ? \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Réponse 1** :\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question2** : \n",
    "\n",
    "Modifier le code de la décente du gradient pour sortir de la boucle (avant que le nombre des itérations soit fini) lorsque l'erreur ne s'améliore plus. \n",
    "\n",
    "Pour ce faire, on calcul un taux de changement et on le compare avec un seuil de changement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO : Définir la déscente du gradient\n",
    "def gradient_descent2(X, Y, theta, nb_iter, learning_rate, affich = True, seuil=0.001):\n",
    "    couts = []\n",
    "    theta1 = theta.copy()\n",
    "    couts.append(J(X, Y, theta1))\n",
    "    \n",
    "    if (affich):\n",
    "        afficher_droite(X, Y, theta1)\n",
    "    \n",
    "    # TODO Définir l'algorithme de la descente de gradient : \n",
    "    \n",
    "        \n",
    "    if(affich):\n",
    "        afficher_droite(X, Y, theta1)\n",
    "\n",
    "    return theta1, couts\n",
    "\n",
    "theta_optimaux, couts = gradient_descent2(X_train, Y_train, theta, NB_ITER, LEARNING_RATE, affich = False)\n",
    "plt.plot(couts)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2 suite** : \n",
    "1. Donner le nombre des itérations nécessaire pour la convergence (à peu près)\n",
    "1. Quel est l'intéret de fixer un nombre des itérations ?\n",
    "1. Quel est l'intéret d'utiliser le taux d'amélioration ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Réponse 2** : \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Question 3** : \n",
    "\n",
    "Essayer de changer les valeurs du learning_rate,\n",
    "Afficher le graphe des coûts (J) par rapport aux différents learning_rate. \n",
    "\n",
    "- Que remarquez-vous ?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO : Tester plusieurs valeurs du learning_rate : \n",
    "\n",
    "#TODO iter_rate = 0.1\n",
    "couts1 = []\n",
    "#TODO iter_rate = 0.05\n",
    "couts2 = []\n",
    "#TODO iter_rate = 0.01\n",
    "couts3 = []\n",
    "\n",
    "# Affichage du graphe des coûts par rapport aux learning_rate :\n",
    "\n",
    "plt.plot(couts1, label = \"0.1\")\n",
    "plt.plot(couts2, label = \"0.05\")\n",
    "plt.plot(couts3, label = \"0.01\")\n",
    "plt.legend()\n",
    "#plt.autoscale()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Réponse 3** : "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 4** : \n",
    "\n",
    "Appliquer la décente du gradient sur les 3 initialisations des paramètres (thetas) : \n",
    "\n",
    "- Initialisation aléatoire \n",
    "- Initialisation à zero \n",
    "- Initialisation à un \n",
    "\n",
    "Est-ce que l'initialisation des paramètres affecte la convergence ?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Initialisation aléatoire \n",
    "couts1 = []\n",
    "\n",
    "# TODO Initialisation à zero \n",
    "couts2 = []\n",
    "\n",
    "# TODO Initialisation à un \n",
    "couts3 = []\n",
    "\n",
    "# Affichage du graphe des coûts par rapport aux learning_rate :\n",
    "\n",
    "plt.plot(couts1, label = \"Aleatoire\")\n",
    "plt.plot(couts2, label = \"zeroes\")\n",
    "plt.plot(couts3, label = \"uns\")\n",
    "plt.legend()\n",
    "#plt.autoscale()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Réponse 4** : \n",
    "\n",
    "- [ ] OUI \n",
    "- [ ] NON"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II Régression Polynomiale \n",
    "\n",
    "Dans cette partie, on essaye d'appliquer une régression polynomiale.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II.1 Régression Linéaire (pour comparaison) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO initialiser theta\n",
    "theta1 = None\n",
    "\n",
    "#Application de la régression linéaire \n",
    "theta1 , couts1 = gradient_descent(X_train, Y_train, theta1, NB_ITER, LEARNING_RATE, affich=False)\n",
    "\n",
    "X_train[:3, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II.2 Régression Polynomiale (degré 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO créer des données similaires à X_train mais avec une colonne X^2\n",
    "X_train2 = None\n",
    "\n",
    "# TODO initialiser theta\n",
    "theta2 = None\n",
    "\n",
    "#Application de la régression linéaire \n",
    "theta2 , couts2 = gradient_descent(X_train2, Y_train, theta2, NB_ITER, LEARNING_RATE, affich=False)\n",
    "\n",
    "X_train2[:3, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II.3 Régression Plynomiale (degré 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO créer des données similaires à X_train2 mais avec une colonne X^3\n",
    "X_train3 = None\n",
    "\n",
    "# TODO initialiser theta\n",
    "theta3 = None\n",
    "\n",
    "#Application de la régression linéaire \n",
    "theta3 , couts3 = gradient_descent(X_train3, Y_train, theta3, NB_ITER, LEARNING_RATE, affich=False)\n",
    "\n",
    "X_train3[:3, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II.4 Comparaison des évolutions des coûts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Affichage du graphe des coûts par rapport aux degré des polynomes :\n",
    "\n",
    "plt.plot(couts1, label = \"a X + b\")\n",
    "plt.plot(couts2, label = \"a X + b X^2 + c\")\n",
    "plt.plot(couts3, label = \"a X + b X^2 + c X^3 + d\")\n",
    "plt.legend()\n",
    "#plt.autoscale()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question** : Que remarquez-vous ?\n",
    "\n",
    "**Réponse** : \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II.5 Comparaison des estimateurs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Affichage des estimations :\n",
    "\n",
    "plt.scatter(X_train[:,1], Y_train, color=\"blue\")\n",
    "plt.plot(X_train[:,1], predire(X_train, theta1), label = \"a X + b\", color=\"red\")\n",
    "plt.plot(X_train[:,1], predire(X_train2, theta2), label = \"a X + b X^2 + c\", color=\"green\")\n",
    "plt.plot(X_train[:,1], predire(X_train3, theta3), label = \"a X + b X^2 + c X^3 + d\", color=\"orange\")\n",
    "plt.legend()\n",
    "#plt.autoscale()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le problème avec le graphe est que les données doivent être ordonnées selon la superficie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trouver les indexes ordonnées \n",
    "idx = X_train[:,1].argsort()\n",
    "\n",
    "X_train = X_train[idx]\n",
    "X_train2 = X_train2[idx]\n",
    "X_train3 = X_train3[idx]\n",
    "Y_train = Y_train[idx] #pas besoin de ça, mais pour avoir les Y alignés avec les X\n",
    "\n",
    "plt.scatter(X_train[:,1], Y_train, color=\"blue\")\n",
    "plt.plot(X_train[:,1], predire(X_train, theta1), label = \"a X + b\", color=\"red\")\n",
    "plt.plot(X_train[:,1], predire(X_train2, theta2), label = \"a X + b X^2 + c\", color=\"green\")\n",
    "plt.plot(X_train[:,1], predire(X_train3, theta3), label = \"a X + b X^2 + c X^3 + d\", color=\"orange\")\n",
    "plt.legend()\n",
    "#plt.autoscale()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II.6 Test des modèles (Performance)\n",
    "\n",
    "Ici, on va tester les modèles : theta1, theta2 et theta3 sur (X_test, Y_test) afin de décider le meilleur modèle dans le cas de prédidication des prix des maisons en utilisant la surface. \n",
    "\n",
    "Pour ce faire, on utilise **Root Mean Squared Error (RMSE)**. Notre fonction de cout est, en réalité, **Mean Squared Error (RMSE)**. Donc, il suffit d'appliquer la racine carrée"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO tester les trois modèles en utilisant la fonction \n",
    "\n",
    "erreur1 = None\n",
    "erreur2 = None\n",
    "erreur3 = None\n",
    "\n",
    "erreur1, erreur2, erreur3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question** : Quel est le meilleur modèle selon la métrique et les données utilisées ?\n",
    "\n",
    "**Réponse** : "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# III Régression lineaire avec scikit-learn \n",
    "\n",
    "\n",
    "On poursuit avec les données des prix maisons : "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III.1 Préparation des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diviser les données \n",
    "from sklearn.model_selection import train_test_split  \n",
    "X = houses.iloc[:, :-1].values # Premières colonnes \n",
    "Y = houses.iloc[:,-1].values # Dernière colonne \n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=0)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III.2 Régression linéaire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entraîner le modèle \n",
    "from sklearn.linear_model import LinearRegression  \n",
    "regressor1 = LinearRegression(normalize=True)  \n",
    "regressor1.fit(X_train, Y_train)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Le coefficient des constantes\n",
    "print(regressor1.intercept_)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Les coéfficients des variables (caractéristiques)\n",
    "print(regressor1.coef_)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X_train, y_train, color=\"blue\")\n",
    "plt.plot(X, regressor1.predict(X), color=\"red\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prédire les valeurs du X_test \n",
    "y_pred = regressor.predict(X_test)  \n",
    "\n",
    "plt.scatter(X_train, y_train, color=\"blue\")\n",
    "plt.scatter(X_test, y_test, color=\"green\")\n",
    "plt.plot(X, regressor.predict(X), color=\"red\")\n",
    "plt.scatter(X_test, y_pred, color=\"violet\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III.3 Régression polynomiale\n",
    "\n",
    "La régression polynomiale est un cas spécial de la régression linéaire. On peut créer de nouvelles caractéristiques dans l'étape de préparation des données en multipliant les valeurs des anciennes caractéristiques. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### III.3.1 Créer des nouvelles caractéristiques\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "poly2 = PolynomialFeatures(degree=2, include_bias=False)\n",
    "\n",
    "X_train2 = poly2.fit_transform(X_train)\n",
    "X_test2 = poly2.fit_transform(X_test)\n",
    "\n",
    "X_train2[:3, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly10 = PolynomialFeatures(degree=10, include_bias=False)\n",
    "\n",
    "X_train10 = poly10.fit_transform(X_train)\n",
    "X_test10 = poly10.fit_transform(X_test)\n",
    "\n",
    "X_train10[:3, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### III.3.2 Entrainer les deux modèles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regrésseur polynomial de degré 2\n",
    "regressor2 = LinearRegression(normalize=True)  \n",
    "regressor2.fit(X_train2, y_train)\n",
    "\n",
    "# Regrésseur polynomial de degré 10\n",
    "regressor10 = LinearRegression(normalize=True)  \n",
    "regressor10.fit(X_train10, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III.5 Evaluation des modèles \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred1 = regressor1.predict(X_test)\n",
    "Y_pred2 = regressor2.predict(X_test2)\n",
    "Y_pred10 = regressor10.predict(X_test10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### III.5.1 Explained variance score\n",
    "\n",
    "$$explained\\_variance(y, ŷ) = 1 - \\frac{Var(y - ŷ)}{Var(y)}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import  explained_variance_score\n",
    "\n",
    "erreur1 = explained_variance_score(Y_test, Y_pred1)\n",
    "erreur2 = explained_variance_score(Y_test, Y_pred2)\n",
    "erreur10 = explained_variance_score(Y_test, Y_pred10)\n",
    "\n",
    "erreur1, erreur2, erreur10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### III.5.2 Mean squared error\n",
    "\n",
    "$$MSE(y, ŷ) = \\frac{1}{nbr\\_echantillons} \\sum\\limits_{i=0}^{nbr\\_echantillons - 1} (y - ŷ)^2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "erreur1 = mean_squared_error(Y_test, Y_pred1)\n",
    "erreur2 = mean_squared_error(Y_test, Y_pred2)\n",
    "erreur10 = mean_squared_error(Y_test, Y_pred10)\n",
    "\n",
    "erreur1, erreur2, erreur10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### III.5.3 Mean absolute error\n",
    "\n",
    "$$MAE(y, ŷ) = \\frac{1}{nbr\\_echantillons} \\sum\\limits_{i=0}^{nbr\\_echantillons - 1} |y - ŷ|$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "erreur1 = mean_absolute_error(Y_test, Y_pred1)\n",
    "erreur2 = mean_absolute_error(Y_test, Y_pred2)\n",
    "erreur10 = mean_absolute_error(Y_test, Y_pred10)\n",
    "\n",
    "erreur1, erreur2, erreur10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### III.5.4 Autres\n",
    "\n",
    "Consulter https://scikit-learn.org/stable/modules/model_evaluation.html#regression-metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III.6 Persistance des modèles \n",
    "\n",
    "Après avoir entrainé un modèle, il est souhaitable de le conserver pour un usage ultérieur sans avoir besoin d'entrainer une deuxième fois. Il y a deux façons de le faire selon la doc de scikit-learn (https://scikit-learn.org/stable/modules/model_persistence.html)\n",
    "\n",
    "- la sérialisation pickle\n",
    "- la sérialisation joblib\n",
    "\n",
    "La deuxième est recommandée par scikit-learn. Après avoir entrainer notre modèle, on le sauvegarde."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import dump, load\n",
    "\n",
    "# Pour sauvegarder le modèle\n",
    "dump(regressor1, \"mon_modele.joblib\")\n",
    "\n",
    "# Pour Récupérer le modèle\n",
    "regressor_past = load(\"mon_modele.joblib\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
