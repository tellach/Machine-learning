{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP06 : Naïve Bayes\n",
    "\n",
    "Tout le monde connait le théorème de Bayes pour calculer la probabilité conditionnelle d'un évennement $A$ sachant un autre $B$: \n",
    "$$ P(A|B) = \\frac{P(A)P(B|A)}{P(B)}$$\n",
    "\n",
    "Pour appliquer ce théorème sur un problème d'appentissage automatique, l'idée est simple ; Etant donné une caractéristique $f$ et la sortie $y$ qui peut avoir la classe $c$ : \n",
    "- Remplacer $A$ par $y=c$\n",
    "- Remplacer $B$ par $f$ \n",
    "On aura l'équation : \n",
    "$$ P(y=c|f) = \\frac{P(y=c)P(f|y=c)}{P(f)}$$\n",
    "\n",
    "On appelle : \n",
    "- $P(y=c|f)$ postérieure \n",
    "- $P(y=c)$ antérieure\n",
    "- $P(f|y=c)$ vraisemblance\n",
    "- $P(f)$ évidence \n",
    "\n",
    "Ici, on estime la probablité d'une classe $c$ sachant une caractéristique $f$ en utilisant des données d'entrainement. Maintenant, on veut estimer la probabilité d'une classe $c$ sachant un vecteur de caractéristiques $\\overrightarrow{f} = \\{f_1, ..., f_L\\}$ : \n",
    "$$ P(y=c|\\overrightarrow{f}) = \\frac{P(y=c)P(\\overrightarrow{f}|y=c)}{P(f)}$$\n",
    "\n",
    "Etant donnée plusieurs classes $c_j$, la classe choisie $\\hat{c}$ est celle avec la probabilité maximale \n",
    "$$\\hat{c} = \\arg\\max\\limits_{c_k} P(y=c_k|\\overrightarrow{f})$$\n",
    "$$\\hat{c} = \\arg\\max\\limits_{c_k} \\frac{P(y=c_k)P(\\overrightarrow{f}|y=c_k)}{P(f)}$$\n",
    "On supprime l'évidence pour cacher le crime : $P(f)$ ne dépend pas de $c_k$ et elle est postive, donc ça ne va pas affecter la fonction $\\max$.\n",
    "$$\\hat{c} = \\arg\\max\\limits_{c_k} P(y=c_k)P(\\overrightarrow{f}|y=c_k)$$\n",
    "\n",
    "Pour calculer $P(\\overrightarrow{f}|y=c_k)$, on va utiliser une properiété naïve (d'où vient le nom Naive Bayes) : on suppose l'indépendence conditionnelle entre les caractéristiques $f_j$. \n",
    "$$\\hat{c} = \\arg\\max\\limits_{c_k} P(y=c_k) \\prod\\limits_{f_j \\in \\overrightarrow{f}} P(f_j|y=c_k)$$\n",
    "\n",
    "Pour éviter la disparition de la probabilité (multiplication et représentation de virgule flottante sur machine), on transforme vers l'espace logarithme.\n",
    "$$\\hat{c} = \\arg\\max\\limits_{c_k} \\log P(y=c_k) + \\sum\\limits_{f_j \\in \\overrightarrow{f}} \\log P(f_j|y=c_k)$$\n",
    "\n",
    "\n",
    "## Avantages \n",
    "\n",
    "Les classifieurs naïfs bayésiens, malgré leur simplicité, ont des points forts:\n",
    "- Ils ont besoin d'une petite quantité de données d'entrainement.\n",
    "- Ils sont très rapides par rapport aux autres classifieurs.\n",
    "- Ils donnent de bons résultats dans le cas de filtrage du courrier indésirable et de classification de documents.\n",
    "\n",
    "## Limites\n",
    "Les classifieurs naïfs bayésiens certes sont populaires à cause de leur simplicité. Mais, une telle simplicité vient avec un coût [référence: Spiderman].\n",
    "- Les probabilités obtenues en utilisant ces classifieurs ne doivent pas être prises au sérieux.\n",
    "- S'il existe une grande corrélation entre les caractéristiques, ils vont donner une mauvaise performance.\n",
    "- Dans le cas des caractéristiques continues (prix, surface, etc.), les données doivent suivre la loi normale.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I- Implémentation\n",
    "\n",
    "Pour estimer la vraisemblance, il y a plusieurs modèles (lois):\n",
    "- Loi multinomiale : pour les caracétristiques nominales\n",
    "- Loi de Bernoulli : lorsqu'on est interressé par l'apparence d'une caractéristique ou non (binaire)\n",
    "- loi normale : pour les caractéristiques numériques\n",
    "\n",
    "Dans ce TP, on va implémenter Naive Bayes pour les caractéristiques nominales (loi multinomiale)\n",
    "\n",
    "### I-1- Les données pour les tests unitaires\n",
    "Ici, on va utiliser le dataset \"jouer\" contenant des caractéristiques nominales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>temps</th>\n",
       "      <th>temperature</th>\n",
       "      <th>humidite</th>\n",
       "      <th>vent</th>\n",
       "      <th>jouer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>ensoleile</td>\n",
       "      <td>chaude</td>\n",
       "      <td>haute</td>\n",
       "      <td>non</td>\n",
       "      <td>non</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>ensoleile</td>\n",
       "      <td>chaude</td>\n",
       "      <td>haute</td>\n",
       "      <td>oui</td>\n",
       "      <td>non</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>nuageux</td>\n",
       "      <td>chaude</td>\n",
       "      <td>haute</td>\n",
       "      <td>non</td>\n",
       "      <td>oui</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>pluvieux</td>\n",
       "      <td>douce</td>\n",
       "      <td>haute</td>\n",
       "      <td>non</td>\n",
       "      <td>oui</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>pluvieux</td>\n",
       "      <td>fraiche</td>\n",
       "      <td>normale</td>\n",
       "      <td>non</td>\n",
       "      <td>oui</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>pluvieux</td>\n",
       "      <td>fraiche</td>\n",
       "      <td>normale</td>\n",
       "      <td>oui</td>\n",
       "      <td>non</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>nuageux</td>\n",
       "      <td>fraiche</td>\n",
       "      <td>normale</td>\n",
       "      <td>oui</td>\n",
       "      <td>oui</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>ensoleile</td>\n",
       "      <td>douce</td>\n",
       "      <td>haute</td>\n",
       "      <td>non</td>\n",
       "      <td>non</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>ensoleile</td>\n",
       "      <td>fraiche</td>\n",
       "      <td>normale</td>\n",
       "      <td>non</td>\n",
       "      <td>oui</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>pluvieux</td>\n",
       "      <td>douce</td>\n",
       "      <td>normale</td>\n",
       "      <td>non</td>\n",
       "      <td>oui</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>ensoleile</td>\n",
       "      <td>douce</td>\n",
       "      <td>normale</td>\n",
       "      <td>oui</td>\n",
       "      <td>oui</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>nuageux</td>\n",
       "      <td>douce</td>\n",
       "      <td>haute</td>\n",
       "      <td>oui</td>\n",
       "      <td>oui</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>nuageux</td>\n",
       "      <td>chaude</td>\n",
       "      <td>normale</td>\n",
       "      <td>non</td>\n",
       "      <td>oui</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>pluvieux</td>\n",
       "      <td>douce</td>\n",
       "      <td>haute</td>\n",
       "      <td>oui</td>\n",
       "      <td>non</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        temps temperature humidite vent jouer\n",
       "0   ensoleile      chaude    haute  non   non\n",
       "1   ensoleile      chaude    haute  oui   non\n",
       "2     nuageux      chaude    haute  non   oui\n",
       "3    pluvieux       douce    haute  non   oui\n",
       "4    pluvieux     fraiche  normale  non   oui\n",
       "5    pluvieux     fraiche  normale  oui   non\n",
       "6     nuageux     fraiche  normale  oui   oui\n",
       "7   ensoleile       douce    haute  non   non\n",
       "8   ensoleile     fraiche  normale  non   oui\n",
       "9    pluvieux       douce  normale  non   oui\n",
       "10  ensoleile       douce  normale  oui   oui\n",
       "11    nuageux       douce    haute  oui   oui\n",
       "12    nuageux      chaude  normale  non   oui\n",
       "13   pluvieux       douce    haute  oui   non"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "\n",
    "\n",
    "jouer = pd.read_csv(\"datasets/jouer.csv\")\n",
    "\n",
    "X_jouer = jouer.iloc[:, :-1].values # Premières colonnes \n",
    "Y_jouer = jouer.iloc[:,-1].values # Dernière colonne \n",
    "\n",
    "# Afficher le dataset \"jouer\"\n",
    "jouer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I-2- Estimation de la probabilité antérieure\n",
    "Etant donné le vecteur de sortie $Y$, on doit calculer la probabilité de chaque classe (différentes valeurs de $Y$)\n",
    "\n",
    "$$p(c_k) = \\frac{|\\{y / y \\in Y \\text{ et } y = c_k\\}|}{|Y|}$$\n",
    "\n",
    "La fonction doit retourner un dictionnaire où la clé est le nom de la classe et la valeur est sa probabilité. Voici, un exemple d'un dictionnaire dans Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(Iris-setosa)= 0.5\n",
      "P(Iris-versicolor)= 0.33\n",
      "P(Iris-virginica)= 0.67\n"
     ]
    }
   ],
   "source": [
    "# Exemple de dictionnaire dans Python \n",
    "d = {}\n",
    "d[\"Iris-setosa\"] = 0.5\n",
    "d[\"Iris-versicolor\"] = 0.33\n",
    "d[\"Iris-virginica\"] = 0.67\n",
    "\n",
    "for c in d: \n",
    "    print(\"P(\" + c + \")= \" + str(d[c]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'non': 0.35714285714285715, 'oui': 0.6428571428571429}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO Réaliser la fonction \n",
    "def P_c(Y): \n",
    "    vals = np.unique(Y)\n",
    "    resultat = {}\n",
    "    for i in range(vals.shape[0]):\n",
    "        resultat[vals[i]] = np.sum(Y == vals[i]) / Y.shape[0]\n",
    "    return resultat\n",
    "\n",
    "# Résultat: {'non': 0.35714285714285715, 'oui': 0.6428571428571429}\n",
    "P_c(Y_jouer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I-3- Entrainement  (loi multinomiale)\n",
    "\n",
    "Notre modèle (notons le par $\\theta_{f_j,C}$) doit garder le nombre des différentes valeurs dans une caractéristique $A$ et le nombre de ces valeurs dans chaque classe.\n",
    "\n",
    "Donc, étant donné un vecteur d'une caractéristique $A$ et un autre des $Y$ respectives, la fonction d'entrainement doit retourner un dictionnaire (notre théta) : \n",
    "- la clé est une valeur $a_v$ de $A$ \n",
    "- la valeur est un autre dictionnaire : \n",
    "   - il doit contenir une clé \"_total_\" dont la valeur est le nombre d'occurence de $a_v$ dans $A$ \n",
    "   - la clé est la classe $c_k$ de $Y$\n",
    "   - la valeur est le nombre d'occurence de $a_v$ respectives à $c_k$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ensoleile': {'_Total_': 5, 'non': 3, 'oui': 2},\n",
       " 'nuageux': {'_Total_': 4, 'non': 0, 'oui': 4},\n",
       " 'pluvieux': {'_Total_': 5, 'non': 2, 'oui': 3}}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO Réaliser cette fonction \n",
    "# Elle génère théta pour une seule caractéristique\n",
    "def entrainer_multi_1(A, Y): \n",
    "    valsA = np.unique(A)\n",
    "    valsY = np.unique(Y)\n",
    "    \n",
    "    resultat = {}\n",
    "    for i in range (valsA.shape[0]):\n",
    "        result = {}\n",
    "        result['_Total_'] = np.sum(A == valsA[i])\n",
    "        for j in range (valsY.shape[0]):\n",
    "            somme = 0\n",
    "            for k in range (A.shape[0]):\n",
    "                if (Y[k] == valsY[j] and A[k] == valsA[i]):\n",
    "                    somme += 1\n",
    "            result[valsY[j]] = somme\n",
    "        resultat[valsA[i]] = result\n",
    "    return resultat\n",
    "\n",
    "# Résultat \n",
    "# {'ensoleile': {'_total_': 5, 'non': 3, 'oui': 2},\n",
    "# 'nuageux': {'_total_': 4, 'non': 0, 'oui': 4},\n",
    "# 'pluvieux': {'_total_': 5, 'non': 2, 'oui': 3}}\n",
    "Theta_jouer_temps = entrainer_multi_1(X_jouer[:, 0], Y_jouer)\n",
    "\n",
    "Theta_jouer_temps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'ensoleile': {'_Total_': 5, 'non': 3, 'oui': 2},\n",
       "  'nuageux': {'_Total_': 4, 'non': 0, 'oui': 4},\n",
       "  'pluvieux': {'_Total_': 5, 'non': 2, 'oui': 3}},\n",
       " {'chaude': {'_Total_': 4, 'non': 2, 'oui': 2},\n",
       "  'douce': {'_Total_': 6, 'non': 2, 'oui': 4},\n",
       "  'fraiche': {'_Total_': 4, 'non': 1, 'oui': 3}},\n",
       " {'haute': {'_Total_': 7, 'non': 4, 'oui': 3},\n",
       "  'normale': {'_Total_': 7, 'non': 1, 'oui': 6}},\n",
       " {'non': {'_Total_': 8, 'non': 2, 'oui': 6},\n",
       "  'oui': {'_Total_': 6, 'non': 3, 'oui': 3}},\n",
       " {'non': 0.35714285714285715, 'oui': 0.6428571428571429}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# La fonction qui entraine Théta sur plusieurs caractéristiques\n",
    "# Rien à programmer ici\n",
    "# Notre théta est une liste des dictionnaires;\n",
    "# chaque dictionnaire contient le théta de la caractéristique respective à la colonne de X\n",
    "# On ajoute les probabilités antérieures des classes à la fin de résultat\n",
    "def entrainer_multi(X, Y): \n",
    "    resultat = []\n",
    "    for i in range(X.shape[1]): \n",
    "        resultat.append(entrainer_multi_1(X[:, i], Y))\n",
    "    resultat.append(P_c(Y))\n",
    "    return resultat\n",
    "\n",
    "Theta_jouer = entrainer_multi(X_jouer, Y_jouer)\n",
    "\n",
    "Theta_jouer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I-4- Estimation de la probabilité de vraissemblance (loi multinomiale)\n",
    "L'équation pour estimer la vraisemblance \n",
    "$$ P(f_j=v|y=c_k) = \\frac{|\\{ y \\in Y / y = c_k \\text{ et } f_j = v\\}|}{|\\{y = c_k\\}|}$$\n",
    "\n",
    "Si, dans le dataset de test, on veut calculer la probabilité d'une valeur $v$ qui n'existe pas dans le dataset d'entrainnement ou qui n'existe pas pour une classe donnée, on aura une probabilité nulle. Ici, on doit appliquer une fonction de lissage qui donne une petite probabilité aux données non vues dans l'entrainnement. Le lissage qu'on va utiliser est celui de Lidstone. Lorsque $\\alpha = 1$ on l'appelle lissage de Laplace.\n",
    "$$ P(f_j=v|y=c_k) = \\frac{|\\{ y \\in Y / y = c_k \\text{ et } f_j = v\\}| + \\alpha}{|\\{y = c_k\\}| + \\alpha * |V|}$$\n",
    "Où: \n",
    "- $\\alpha$ est une valeur donnée \n",
    "- $V$ est l'ensemble des différentes valeurs de $f_j$ (le vocabulaire)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.3333333333333333, 0.08333333333333333)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO compléter cette fonction\n",
    "def P_vraiss_multi(Theta_j, v, c, alpha=1.): \n",
    "    len_V = len(Theta_j) # La taille du vocabulaire\n",
    "    nbr_c = 0\n",
    "    for i in Theta_j.keys() :\n",
    "        nbr_c += Theta_j[i][c]\n",
    "    return ((Theta_j[v][c] if (v in Theta_j.keys()) else 0) + alpha) / (nbr_c + alpha * len_V )\n",
    "\n",
    "# La probabilité de jouer si temps = pluvieux \n",
    "# P(temps = pluvieux | jouer=oui) = (nbr(temps=pluvieux et jouer=oui)+alpha)/(nbr(jour=oui) + alpha * nbr_diff(temps)))\n",
    "# P(temps = pluvieux | jouer=oui) = (3 + 1)/(9 + 3) ==> 3 est le nombre de différentes valeurs de temps (entrainnement)\n",
    "# P(temps = pluvieux | jouer=oui) = 4/12 ==> 0.33333333333333333333333333333333333~\n",
    "\n",
    "# La probabilité de jouer si temps = neigeux \n",
    "# P(temps = neigeux | jouer=oui) = (nbr(temps=neigeux et jouer=oui)+alpha)/(nbr(jouer=oui) + alpha * nbr_diff(temps)))\n",
    "# P(temps = neigeux | jouer=oui) = (0 + 1)/(9 + 3) ==> 3 est le nombre de différentes valeurs de temps (entrainnement)\n",
    "# P(temps = neigeux | jouer=oui) = 1/13 ==> 0.0833333333333333333333333333333333333~\n",
    "\n",
    "\n",
    "P_vraiss_multi(Theta_jouer_temps, \"pluvieux\", \"oui\"), P_vraiss_multi(Theta_jouer_temps, \"neigeux\", \"oui\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I-5- Prédiction de la classe (loi multinomiale)\n",
    "Revenons maintenant à notre équation de prédiction \n",
    "$$\\hat{c} = \\arg\\max\\limits_{c_k} \\log P(y=c_k) + \\sum\\limits_{f_j \\in \\overrightarrow{f}} \\log P(f_j|y=c_k)$$\n",
    "\n",
    "Ici, vous devez prédire un seule échantillon $x$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(('oui', -4.102643365036796), ('oui', -3.6608106127577567))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO compléter ce code\n",
    "# Pour récupérer le théta de la caractéristique n°0 : Theta[0]\n",
    "# anter est un booléen, si il est False, on ne compte pas la probabilité antérieure P(y = c_k)\n",
    "def predire(x, Theta, alpha=1., anter=True): \n",
    "    #print(Theta)\n",
    "    c_opt = \"\" # la classe optimale\n",
    "    p_c = Theta[-1] #les classes et leurs probabilités antérieures\n",
    "    if not anter: # si on ne veut pas ajouter les probabiliés antérieures\n",
    "        p_c = dict.fromkeys(p_c, 1.) # on définit le tous en 1; log(1) = 0\n",
    "    max_log_p = np.NINF # - infinity \n",
    "    # compléter ici\n",
    "    for i in p_c.keys() :\n",
    "        log_p = np.log(p_c[i])\n",
    "        k = 0\n",
    "        for j in x :\n",
    "            log_p += np.log(P_vraiss_multi(Theta[k], j, i, alpha))\n",
    "            k += 1\n",
    "        if (log_p > max_log_p):\n",
    "            c_opt = i\n",
    "            max_log_p = log_p\n",
    "    return c_opt, max_log_p\n",
    "\n",
    "# Résultat: (('oui', -4.102643365036796), ('oui', -3.6608106127577567))\n",
    "predire([\"pluvieux\", \"fraiche\", \"normale\", \"oui\"], Theta_jouer), predire([\"pluvieux\", \"fraiche\", \"normale\", \"oui\"], Theta_jouer, anter=False) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I-7- Regrouper en une classe (loi multinomiale)\n",
    "\n",
    "**Rien à programmer ici, il y a une petite analyse**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NBMultinom(object): \n",
    "    \n",
    "    def __init__(self, alpha=1.): \n",
    "        self.alpha = alpha\n",
    "        \n",
    "    def entrainer(self, X, Y):\n",
    "        self.Theta = entrainer_multi(X, Y)\n",
    "    \n",
    "    def predire(self, X, anter=True, prob=False): \n",
    "        Y_pred = []\n",
    "        for i in range(len(X)): \n",
    "            c, p = predire(X[i,:], self.Theta, alpha=self.alpha, anter=anter)\n",
    "            if prob:\n",
    "                Y_pred.append(p)\n",
    "            else:\n",
    "                Y_pred.append(c)\n",
    "        return Y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On va entrainer un modèle en utilisant notre imlémentation avec et sans probabilité antérieure. \n",
    "Normalement, on doit tester sur des données non vues (des données qu'on n'a pas utilisé pour l'entrainement). Mais, ici, on va tester sur les mêmes données d'entrainement afin de savoir si le modèle a bien représenté ce dataset ou non (calculer l'erreur) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Notre modèle avec probabilité antérieure (a priori)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         non       0.80      1.00      0.89         4\n",
      "         oui       1.00      0.90      0.95        10\n",
      "\n",
      "    accuracy                           0.93        14\n",
      "   macro avg       0.90      0.95      0.92        14\n",
      "weighted avg       0.94      0.93      0.93        14\n",
      "\n",
      "Notre modèle sans probabilité antérieure (a priori)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         non       0.80      0.67      0.73         6\n",
      "         oui       0.78      0.88      0.82         8\n",
      "\n",
      "    accuracy                           0.79        14\n",
      "   macro avg       0.79      0.77      0.78        14\n",
      "weighted avg       0.79      0.79      0.78        14\n",
      "\n"
     ]
    }
   ],
   "source": [
    "notre_modele = NBMultinom()\n",
    "notre_modele.entrainer(X_jouer, Y_jouer)\n",
    "Y_notre_ant = notre_modele.predire(X_jouer)\n",
    "Y_notre_sans_ant = notre_modele.predire(X_jouer, anter=False)\n",
    "\n",
    "# Ici, ce n'ai pas la peine d'exécuter plusieurs fois\n",
    "# puisque le résultat sera le même \n",
    "\n",
    "# Le rapport de classification\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(\"Notre modèle avec probabilité antérieure (a priori)\")\n",
    "print(classification_report(Y_notre_ant, Y_jouer))\n",
    "\n",
    "print(\"Notre modèle sans probabilité antérieure (a priori)\")\n",
    "print(classification_report(Y_notre_sans_ant, Y_jouer))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Analyser les résultats** <br>\n",
    "- On remarque que le modèle avec probabilité antérieure donne de meilleurs résultats que modèle sans probabilité antérieure. le premier modèle a une meilleur precison , recall et f1 score que le deuxième modèle. Le premier modèle converge donc mieux que le second car sans la probabilité antérieure, le modèle considère que toutes les caractéristiques ont le même nombre d'observations (échantillons)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II- Détection de spam \n",
    "\n",
    "Ici, on va essayer d'appliquer l'apprentissage automatique sur la détection de spam. \n",
    "Chaque message dans le dataset est représenté en utilisant un modèle \"Sac à mots\" (BoW : Bag of Words).\n",
    "Dans l'entrainement, on récupère les différents mots qui s'apparaissent dans les messages. \n",
    "Chaque mot va être considéré comme une caractéristique. \n",
    "Donc, pour chaque message, la valeur de la caractéristique est la fréquence de son mot dans le message. \n",
    "Par exemple, si le mot \"good\" apparait 3 fois dans le message, donc la caractéristique \"good\" aura la valeur 3 dans ce message.\n",
    "\n",
    "Notre implémentation n'est pas adéquate pour la nature de ce problème. \n",
    "Dans Scikit-learn, le [sklearn.naive_bayes.CategoricalNB](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.CategoricalNB.html) est similaire à notre implémentation. \n",
    "L'algorithme adéquat pour ce type de problème est [sklearn.naive_bayes.MultinomialNB](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html).\n",
    "\n",
    "Le dataset utilisé est [SMS Spam Collection Dataset](https://www.kaggle.com/uciml/sms-spam-collection-dataset).\n",
    "Les algorithmes comparés :\n",
    "- Naive Bayes\n",
    "- Arbre de décision\n",
    "- Regression logistique \n",
    "\n",
    "### II-1- Préparation de données\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>texte</th>\n",
       "      <th>classe</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               texte classe\n",
       "0  Go until jurong point, crazy.. Available only ...    ham\n",
       "1                      Ok lar... Joking wif u oni...    ham\n",
       "2  Free entry in 2 a wkly comp to win FA Cup fina...   spam\n",
       "3  U dun say so early hor... U c already then say...    ham\n",
       "4  Nah I don't think he goes to usf, he lives aro...    ham"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages = pd.read_csv(\"datasets/spam.csv\", encoding=\"latin-1\")\n",
    "messages = messages.rename(columns={\"v1\": \"classe\", \"v2\": \"texte\"})\n",
    "messages = messages.filter([\"texte\", \"classe\"])\n",
    "\n",
    "messages.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II-2- Entrainement et test des modèles sur plusieurs exécutions \n",
    "\n",
    "Afin de satisfaire un étudiant qui réclame toujours sur le manque des données, nous avons décidé de comparer les algorithmes sur plusieurs excécutions (runs). \n",
    "\n",
    "**Rien à analyser ici**\n",
    "\n",
    "**P.S.** timeit.default_timer() est dépendante du système d'exploitation. Aussi, elle peut être affectée par d'autre processus en parallèle. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'naive_bayes': [0.008581199999753153,\n",
       "  0.00817430000006425,\n",
       "  0.008171500000571541,\n",
       "  0.012063600000146835,\n",
       "  0.008729700000003504,\n",
       "  0.014888100000462146,\n",
       "  0.007862899999963702],\n",
       " 'arbre_decision': [0.10807889999978215,\n",
       "  0.09442619999936142,\n",
       "  0.0900523999998768,\n",
       "  0.10902809999970486,\n",
       "  0.12166809999962425,\n",
       "  0.11890270000003511,\n",
       "  0.12075439999989612],\n",
       " 'reg_log': [0.08972889999949984,\n",
       "  0.059421300000394695,\n",
       "  0.05611010000029637,\n",
       "  0.0707167999999001,\n",
       "  0.05657049999990704,\n",
       "  0.062007799999264535,\n",
       "  0.07169219999923371]}"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "#from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import timeit\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "\n",
    "NBR_RUN = 7\n",
    "\n",
    "temps_train = {\n",
    "    \"naive_bayes\" : [],\n",
    "    \"arbre_decision\": [],\n",
    "    \"reg_log\": []\n",
    "}\n",
    "\n",
    "temps_test = {\n",
    "    \"naive_bayes\" : [],\n",
    "    \"arbre_decision\": [],\n",
    "    \"reg_log\": []\n",
    "}\n",
    "\n",
    "perf = {\n",
    "    \"naive_bayes_P\" : [],\n",
    "    \"arbre_decision_P\": [],\n",
    "    \"reg_log_P\": [], \n",
    "    \"naive_bayes_R\" : [],\n",
    "    \"arbre_decision_R\": [],\n",
    "    \"reg_log_R\": []\n",
    "}\n",
    "\n",
    "\n",
    "for run in range(NBR_RUN): \n",
    "    # prétaitement des données\n",
    "    msg_train, msg_test, Y_train, Y_test = train_test_split(messages[\"texte\"],messages[\"classe\"],test_size=0.2)\n",
    "    count_vectorizer = CountVectorizer()\n",
    "    X_train = count_vectorizer.fit_transform(msg_train)\n",
    "    X_test = count_vectorizer.transform(msg_test)\n",
    "    \n",
    "    # ==================================\n",
    "    # ENTRAINEMENT \n",
    "    # ==================================\n",
    "    \n",
    "    #entrainement Naive Bayes\n",
    "    naive_bayes = MultinomialNB()\n",
    "    temps_debut = timeit.default_timer()\n",
    "    naive_bayes.fit(X_train, Y_train)\n",
    "    temps_train[\"naive_bayes\"].append(timeit.default_timer() - temps_debut)\n",
    "    \n",
    "    #entrainement CART\n",
    "    arbre_decision = DecisionTreeClassifier()\n",
    "    temps_debut = timeit.default_timer()\n",
    "    arbre_decision.fit(X_train, Y_train)\n",
    "    temps_train[\"arbre_decision\"].append(timeit.default_timer() - temps_debut)\n",
    "    \n",
    "    #entrainement Régression logitique\n",
    "    reg_log = LogisticRegression(solver=\"lbfgs\") #solver=sag est plus lent; donc j'ai choisi le plus rapide\n",
    "    temps_debut = timeit.default_timer()\n",
    "    reg_log.fit(X_train, Y_train)\n",
    "    temps_train[\"reg_log\"].append(timeit.default_timer() - temps_debut)\n",
    "    \n",
    "    # ==================================\n",
    "    # TEST \n",
    "    # ==================================\n",
    "    \n",
    "    #test Naive Bayes\n",
    "    temps_debut = timeit.default_timer()\n",
    "    Y_naive_bayes = naive_bayes.predict(X_test)\n",
    "    temps_test[\"naive_bayes\"].append(timeit.default_timer() - temps_debut)\n",
    "    \n",
    "    \n",
    "    #test CART\n",
    "    temps_debut = timeit.default_timer()\n",
    "    Y_arbre_decision = arbre_decision.predict(X_test)\n",
    "    temps_test[\"arbre_decision\"].append(timeit.default_timer() - temps_debut)\n",
    "    \n",
    "    #test Régression logitique\n",
    "    temps_debut = timeit.default_timer()\n",
    "    Y_reg_log = reg_log.predict(X_test)\n",
    "    temps_test[\"reg_log\"].append(timeit.default_timer() - temps_debut)\n",
    "    \n",
    "    # ==================================\n",
    "    # PERFORMANCE \n",
    "    # ==================================\n",
    "    # Ici, on va considérer une classification binaire avec une seule classe \"spam\" \n",
    "    # On ne juge pas le classifieur sur sa capacité de détecter les non spams\n",
    "    \n",
    "    perf[\"naive_bayes_P\"].append(precision_score(Y_test, Y_naive_bayes, pos_label=\"spam\"))\n",
    "    perf[\"arbre_decision_P\"].append(precision_score(Y_test, Y_arbre_decision, pos_label=\"spam\"))\n",
    "    perf[\"reg_log_P\"].append(precision_score(Y_test, Y_reg_log, pos_label=\"spam\"))\n",
    "    \n",
    "    perf[\"naive_bayes_R\"].append(recall_score(Y_test, Y_naive_bayes, pos_label=\"spam\"))\n",
    "    perf[\"arbre_decision_R\"].append(recall_score(Y_test, Y_arbre_decision, pos_label=\"spam\"))\n",
    "    perf[\"reg_log_R\"].append(recall_score(Y_test, Y_reg_log, pos_label=\"spam\"))\n",
    "    \n",
    "    \n",
    "\n",
    "temps_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II-3- Analyse du temps d'apprentissage \n",
    "\n",
    "Combien de temps chaque algorithme prend pour entrainer le même dataset d'entrainement\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>naive_bayes</th>\n",
       "      <th>arbre_decision</th>\n",
       "      <th>reg_log</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.008581</td>\n",
       "      <td>0.108079</td>\n",
       "      <td>0.089729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.008174</td>\n",
       "      <td>0.094426</td>\n",
       "      <td>0.059421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.008172</td>\n",
       "      <td>0.090052</td>\n",
       "      <td>0.056110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.012064</td>\n",
       "      <td>0.109028</td>\n",
       "      <td>0.070717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.008730</td>\n",
       "      <td>0.121668</td>\n",
       "      <td>0.056570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.014888</td>\n",
       "      <td>0.118903</td>\n",
       "      <td>0.062008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.007863</td>\n",
       "      <td>0.120754</td>\n",
       "      <td>0.071692</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   naive_bayes  arbre_decision   reg_log\n",
       "0     0.008581        0.108079  0.089729\n",
       "1     0.008174        0.094426  0.059421\n",
       "2     0.008172        0.090052  0.056110\n",
       "3     0.012064        0.109028  0.070717\n",
       "4     0.008730        0.121668  0.056570\n",
       "5     0.014888        0.118903  0.062008\n",
       "6     0.007863        0.120754  0.071692"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(temps_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Analyser** <br>\n",
    "- On remarque que le modèle naive bayes a un temps d'apprentissage court par rapport à la régression logistique et au modèle d'arbre de décision. Donc le modèle naive bayes converge plus rapidement que les deux autres modelés (arbre de décision et la régression logistique)\n",
    "- On remarque aussi que la régression logistique présente de bons temps d'entrainement, contrairement aux arbres de décision qui prennent un temps plus élevé , cela est dû au nombre important de caractéristique."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II-4- Analyse du temps de test \n",
    "\n",
    "Combien de temps chaque algorithme prend pour prédir les classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>naive_bayes</th>\n",
       "      <th>arbre_decision</th>\n",
       "      <th>reg_log</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.000197</td>\n",
       "      <td>0.000399</td>\n",
       "      <td>0.000150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.000194</td>\n",
       "      <td>0.000385</td>\n",
       "      <td>0.000111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.000328</td>\n",
       "      <td>0.000525</td>\n",
       "      <td>0.000148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.000196</td>\n",
       "      <td>0.000351</td>\n",
       "      <td>0.000090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.000362</td>\n",
       "      <td>0.000403</td>\n",
       "      <td>0.000101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.000336</td>\n",
       "      <td>0.000482</td>\n",
       "      <td>0.000141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.000197</td>\n",
       "      <td>0.000366</td>\n",
       "      <td>0.000092</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   naive_bayes  arbre_decision   reg_log\n",
       "0     0.000197        0.000399  0.000150\n",
       "1     0.000194        0.000385  0.000111\n",
       "2     0.000328        0.000525  0.000148\n",
       "3     0.000196        0.000351  0.000090\n",
       "4     0.000362        0.000403  0.000101\n",
       "5     0.000336        0.000482  0.000141\n",
       "6     0.000197        0.000366  0.000092"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(temps_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Analyser** <br>\n",
    "- On remarque que la régression logistique présente les meilleur résultats car il s'agit d'une régression binaire (spam/non spam) donc un simple calcule d'une valeur en utilisant un vecteur de theta et la comparer avec un seuil.\n",
    "- Le modèle de naive bayes vient en deuxième position, car pour cela il faut juste calculer la vraisemblance de chacune des caractéristiques et l'antérieure des valeurs des y.\n",
    "- Les arbres de décision présentent le plus grand temps de test. Cela est dû au nombre de caractéristique(chaque mot présente une) donc l'arbre est trop profond pour le parcourir."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II-5- Analyse de la performance \n",
    "\n",
    "Ici, on compare les modèles en se basant sur leurs capacités à détecter le spam. \n",
    "On va utiliser la précision et le rappel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>arbre_decision_P</th>\n",
       "      <th>naive_bayes_P</th>\n",
       "      <th>reg_log_P</th>\n",
       "      <th>arbre_decision_R</th>\n",
       "      <th>naive_bayes_R</th>\n",
       "      <th>reg_log_R</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.932836</td>\n",
       "      <td>0.984848</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.886525</td>\n",
       "      <td>0.921986</td>\n",
       "      <td>0.886525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.899281</td>\n",
       "      <td>0.977778</td>\n",
       "      <td>0.976744</td>\n",
       "      <td>0.874126</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.881119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.887218</td>\n",
       "      <td>0.976378</td>\n",
       "      <td>0.983333</td>\n",
       "      <td>0.855072</td>\n",
       "      <td>0.898551</td>\n",
       "      <td>0.855072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.873134</td>\n",
       "      <td>0.992308</td>\n",
       "      <td>0.961240</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>0.902098</td>\n",
       "      <td>0.867133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.909774</td>\n",
       "      <td>0.968750</td>\n",
       "      <td>0.983607</td>\n",
       "      <td>0.889706</td>\n",
       "      <td>0.911765</td>\n",
       "      <td>0.882353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.904110</td>\n",
       "      <td>0.979452</td>\n",
       "      <td>0.992537</td>\n",
       "      <td>0.862745</td>\n",
       "      <td>0.934641</td>\n",
       "      <td>0.869281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.864865</td>\n",
       "      <td>0.971223</td>\n",
       "      <td>0.984848</td>\n",
       "      <td>0.882759</td>\n",
       "      <td>0.931034</td>\n",
       "      <td>0.896552</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   arbre_decision_P  naive_bayes_P  reg_log_P  arbre_decision_R  \\\n",
       "0          0.932836       0.984848   1.000000          0.886525   \n",
       "1          0.899281       0.977778   0.976744          0.874126   \n",
       "2          0.887218       0.976378   0.983333          0.855072   \n",
       "3          0.873134       0.992308   0.961240          0.818182   \n",
       "4          0.909774       0.968750   0.983607          0.889706   \n",
       "5          0.904110       0.979452   0.992537          0.862745   \n",
       "6          0.864865       0.971223   0.984848          0.882759   \n",
       "\n",
       "   naive_bayes_R  reg_log_R  \n",
       "0       0.921986   0.886525  \n",
       "1       0.923077   0.881119  \n",
       "2       0.898551   0.855072  \n",
       "3       0.902098   0.867133  \n",
       "4       0.911765   0.882353  \n",
       "5       0.934641   0.869281  \n",
       "6       0.931034   0.896552  "
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(perf, columns = [\"arbre_decision_P\", \"naive_bayes_P\", \"reg_log_P\", \"arbre_decision_R\", \"naive_bayes_R\", \"reg_log_R\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Analyser** <br>\n",
    "- On remarque que les deux modèles naive bayes et régression logistique ont la même précision contrairement au modèle d'arbre de décision qui est moins précis que les deux autres car l'élagage de l'arbre de décision peut négliger certaines valeurs clés des données d'entraînement, ce qui peut conduire à la précision du modèle.\n",
    "- On remarque que le modèle naive bayes a un meilleur rappel que les deux autres modèles (régression logistique et les arbres de décisions) car la règle d'indépendance est appliqué sur cette exemple donc le modèle naive bayes est le mieux adapté pour le filtrage du spam .\n",
    "\n",
    "**Conclusion** <br>\n",
    "- Le modèle Naive bayes est facile et rapide de prédire la classe de l’ensemble de données de test . Il fonctionne également bien dans la prédiction multi-classes.\n",
    "- Lorsque l'hypothèse d'indépendance est vérifiée, un classificateur Naive Bayes fonctionne mieux que d'autres modèles comme la régression logistique et vous avez besoin de moins de données d'entraînement.\n",
    "- Il fonctionne bien en cas de variables d'entrée catégorielles par rapport aux variables numériques. Pour la variable numérique, une distribution normale est supposée (courbe en cloche, qui est une hypothèse forte)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
